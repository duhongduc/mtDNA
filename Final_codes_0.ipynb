{"cells":[{"cell_type":"markdown","metadata":{"id":"195bMV1d0fL9"},"source":["## **SUMMARY CODE**\n","***Author: Vy Phung***  \n","  - This file summarizes and explains the methods and codes that I wrote and used to retrieve the raw mtDNA sequences from NCBI gen bank, then clean these raw data to get the final Dataset 3 (which only includes complete genomes sequences), and finally use the summary information of each sequence to create the Isolate Explanation table.\n","\n","  - For the purpose of simplifying and making the codes easier to run and read, I saved our codes in the github link below where I just called the packages and functions everytime I ran. To know more about the specific details of the codes, you can look up the github link below: https://github.com/duhongduc/Haplo."]},{"cell_type":"markdown","metadata":{"id":"AyVUgrJFLvyl"},"source":["Caveats:\n","- Because the time I ran these codes to get raw data was June 2023, if you intend to reuse our codes and run them, there might be more published raw data. When I started to write this summary code file (May 2024), I re-ran these codes, and used the same keywords to search for the sequences to make sure the codes still work. I accidentally realized that more data have been published when I compared a number of sequences of re-ran Dataset 3 with that of the original Dataset 3 (4932 sequences). Therefore, in the scenerio of this summary code file, and also our paper, I still summarize the same methods how I got to Dataset 3, but at Dataset 3 section, I created another function to filter only original 4932 sequences that I used in our paper.\n","- The reason I used Google Colab is because Google Colab and Google Drive are linked to each other, and I can access to google drive folder to get and also save our data from this. Here is the google drive folder that I used in our paper:\n","https://drive.google.com/drive/folders/15OPBImGAG51vukHfADV3-zG8RKU8A9fe?usp=drive_link. </br>\n","\n","Everytime you see the below codes saving the files at any folders, the locations of the folders are the same as the location in the google drive."]},{"cell_type":"markdown","metadata":{"id":"aGv8tuieL07w"},"source":["\n","**CONTENTS:**\n","1. Get Data from NCBI\n","* Setting up\n","* Entrez Direct\n","* Missing Data\n","2. Data wrangling\n","* Dataset 1\n","* Dataset 2\n","* Dataset 3\n","3. Tables\n","* Isolate Explanation Table\n","* Table 1\n","* Table 2\n","* Table 3 and its subtables"]},{"cell_type":"markdown","metadata":{"id":"Q42i00Uq017m"},"source":["### **1. Get Data from NCBI**"]},{"cell_type":"markdown","metadata":{"id":"NsmvEgCt01sb"},"source":["#### **Setting up**"]},{"cell_type":"markdown","metadata":{"id":"5JJ6sV6gVdi8"},"source":["Firstly, I accessed to our own google drive by running a cell code below\n","(Hit Shift + Enter to run any box cell code)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9adCWjqL6Ew","outputId":"49686e83-049f-48b9-ea4c-1f1296bf1a27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Access to my google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Bu6ZFY7RxyR","outputId":"7f3874af-1a02-4ead-e2c0-12e102181902"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive\n"]}],"source":["# After accessing, run this cell code to move into my google drive directory\n","%cd /content/drive/MyDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfknG_uASDIU","outputId":"b0347732-6458-4605-c56c-9b3ee7a9a8cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Haplogroup'...\n","remote: Enumerating objects: 88, done.\u001b[K\n","remote: Counting objects: 100% (20/20), done.\u001b[K\n","remote: Compressing objects: 100% (18/18), done.\u001b[K\n","remote: Total 88 (delta 3), reused 17 (delta 2), pack-reused 68\u001b[K\n","Receiving objects: 100% (88/88), 50.73 KiB | 2.21 MiB/s, done.\n","Resolving deltas: 100% (21/21), done.\n"]}],"source":["# Then run this cell code to clone the main directory Haplogroup from the github link into my google drive folder, so that everytime I run the code, I just call them from this google drive\n","! git clone https://github.com/vy-phung/Haplogroup.git"]},{"cell_type":"markdown","metadata":{"id":"1CQ5RaLv5DXN"},"source":["#### **Entrez Direct**"]},{"cell_type":"markdown","metadata":{"id":"-j9r_cYm3ZR4"},"source":["- To retrieve mtDNA sequences of 11 countries in South East Asia, I used [Entrez Direct](https://www.ncbi.nlm.nih.gov/books/NBK179288/) to search for the common keywords below: </br>\n","`Homo sapiens AND mitochondrion AND <Country Name>`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1ADFF2cjA6_","outputId":"960533de-636d-43e6-eb3a-b7e90c1276f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Entrez Direct has been successfully downloaded and installed.\n","\n","In order to complete the configuration process, please execute the following:\n","\n","  echo \"export PATH=/root/edirect:\\${PATH}\" >> ${HOME}/.bashrc\n","\n","or manually edit the PATH variable assignment in your .bashrc file.\n","\n","Would you like to do that automatically now? [y/N]\n","y\n","OK, done.\n","\n","To activate EDirect for this terminal session, please execute the following:\n","\n","export PATH=${HOME}/edirect:${PATH}\n","\n"]}],"source":["# Download Entrez Direct\n","!sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\n","!export PATH=${HOME}/edirect:${PATH}"]},{"cell_type":"markdown","metadata":{"id":"oRm3AOxzv-TJ"},"source":["Below is a bash script file `./Haplogroup/finalCodes/bashScriptCodes/downloadDataNCBI.sh` I wrote to download and then save the data.\n","\n","- \"DataListName\": /content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt </br>\n","(A file \"countries.txt\" contains the names of 11 countries: Brunei, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Thailand, Timor-Leste, Viet Nam)\n","\n","- \"NameOfSaveFolder\": /content/drive/MyDrive/RetrieveData/OldCountryFasta </br>(\"OldCountryFasta\" is the folder I saved the downloaded data)\n","\n","The downloaded data from each country was saved to a big fasta file named after that country."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cz-sxXyTX5Pm"},"outputs":[],"source":["# run this cell code to run the bash script to get the data from entrez direct\n","''' example code:\n","! source ./Haplogroup/finalCodes/bashScriptCodes/downloadDataNCBI.sh; download \"DataListName\" \"NameOfSaveFolder\"\n","'''\n","! source ./Haplogroup/finalCodes/bashScriptCodes/downloadDataNCBI.sh; download /content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt /content/drive/MyDrive/RetrieveData/OldCountryFasta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Gli-xf8o8xu"},"outputs":[],"source":["# check the number of sequences in each country fasta file after downloading\n","%%bash\n","DataList=/content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt\n","Field_Separator=$IFS\n","IFS=,\n","for val in `cat $DataList`\n","do echo $val; cat /content/drive/MyDrive/RetrieveData/OldCountryFasta/$val.fasta | grep \">\" | wc -l; done"]},{"cell_type":"markdown","metadata":{"id":"DuAovrI75VoA"},"source":["#### **Missing data**\n","\n","After gaining the data from the above common keywords, I realized that there Ire still some more missing data and I could get these missing ones if I had searched the keywords specifically. Below is how I got more data for each specific country. I saved the missing data of that country directly to an existing big fasta file having that country's name."]},{"cell_type":"markdown","metadata":{"id":"UWpj4t5mBh1y"},"source":["**Myanmar** </br>\n","New Data:\n","- 21219640.Inland post-glacial dispersal in East Asia revealed by mitochondrial haplogroup M9a'b: Myanmar, Vietnam (filter them): keywords for Myanmar: HM346895, HM346896\n","- 24467713.Summerer et al. (2014).txt: Myanmar: missing 327 small coding region JX288765-JX289091 among 371 files of this article (keywords: Large-scale mitochondrial DNA analysis in Southeast Asia reveals evolutionary effects of cultural isolation in the multi-ethnic population of Myanmar)\n","- 25826227.Li et al. (2015).txt: all 937 files for Myanmar but only 92 files exist (keywords: Ancient inland human dispersals from Myanmar into interior East Asia since the Late Pleistocene)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AtGCcc8-MUn5"},"outputs":[],"source":["%%bash\n","# downloading missing data of Myanmar\n","for str in \"HM346895\" \"HM346896\" \"Large-scale mitochondrial DNA analysis in Southeast Asia reveals evolutionary effects of cultural isolation in the multi-ethnic population of Myanmar\" \"Ancient inland human dispersals from Myanmar into interior East Asia since the Late Pleistocene\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Myanmar\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"oi_z2gyU3WAI"},"source":["**Philippines** </br>\n","New data:\n","+ 21281460.Loo et al. (2011).txt: all 46 files for Philip but only exist 12 (keyword: Genetic affinities betIen the Yami tribe people of Orchid Island and the Philippine Islanders of the Batanes archipelago and Philippines)\n","+ 21796613.Scholes et al. (2011).txt: 60 files for Philip but only exist 9 (keywords: Genetic diversity and evidence for population admixture in Batak Negritos from Palawan)\n","+ Philippines: '28535779.Carriers of mitochondrial DNA macrohaplogroup R colonized Eurasia.Larruga et al. (2017)': keywords: \"Carriers of mitochondrial DNA macrohaplogroup R colonized Eurasia AND Philippines\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIIe36S-SxLK"},"outputs":[],"source":["%%bash\n","# downloading missing data of Philippines\n","for str in \"Genetic affinities betIen the Yami tribe people of Orchid Island and the Philippine Islanders of the Batanes archipelago and Philippines\" \"Genetic diversity and evidence for population admixture in Batak Negritos from Palawan\" \"Carriers of mitochondrial DNA macrohaplogroup R colonized Eurasia AND Philippines\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Philippines\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"IDwv9REX4WE4"},"source":["**Singapore** </br>\n","New data:\n","+ 36382667.Sui et al. (2023).txt: there are 7 and already exist 5 so there are 2 more but no idea if these 2 ref seqs are from Singapore or not (keywords: Death associated protein‑3 (DAP3) and DAP3 binding cell death enhancer‑1 (DELE1) in human colorectal cancer, and their impacts on clinical outcome and chemoresistance)\n","+ 37025097.Zhao et al. (2023).txt: 14 files and already exist 11 files but don’t know about the other 3 belonging to Singapore or not"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxV-To-dPJ1x"},"outputs":[],"source":["%%bash\n","# downloading missing data of Singapore\n","for str in \"Death associated protein‑3 (DAP3) and DAP3 binding cell death enhancer‑1 (DELE1) in human colorectal cancer, and their impacts on clinical outcome and chemoresistance\" \"Effect of COP1 in Promoting the Tumorigenesis of Gastric Cancer by Down-Regulation of CDH18 via PI3K/AKT Signal Pathway\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Singapore\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"spNQKJDa4rSQ"},"source":["**Thailand** </br>\n","New data:\n","+ 11310578.Mitochondrial DNA polymorphisms in Thailand.Fucharoen et al. (2001)\n","+ 19148289.The Peopling of Korea Revealed by Analyses of Mitochondrial DNA and Y-Chromosomal Markers.Jin et al. (2009): Thai (80, keyword: The Peopling of Korea Revealed by Analyses of Mitochondrial DNA and Y-Chromosomal Markers and Thailand), Vietnam (84, keyword: Viet Nam)\n","+ 27837350.Complete mitochondrial genomes of Thai and Lao populations indicate an ancient origin of Austroasiatic groups and demic diffusion in the spread of Tai–Kadai languages. Kutanan et al. (2017): Thai, Lao (search on the table of isolate name): total 1234\n","+ 32304863.A Matrilineal Genetic Perspective of Hanging Coffin\n","Custom in Southern China and Northern Thailand (its old name is Unpublished.The Population History and Cultural Dispersal Pattern of Hanging.Zhang et al. (2020))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCkL9rFraIKW"},"outputs":[],"source":["# Special case\n","'''27837350.Complete mitochondrial genomes of Thai and Lao populations indicate an ancient origin of Austroasiatic groups and demic diffusion in the\n","spread of Tai–Kadai languages. Kutanan et al. (2017):I used the title of this paper to download the data, but the data of countries Thailand and Lao are mixed (search on the table of isolate name):\n","total 1234: Laos: LUA101-LUA149: LA1 + VIE101-VIE149: LA2; the others are Thai'''\n","! ${HOME}/edirect/esearch -db nucleotide -query \"Complete mitochondrial genomes of Thai and Lao populations indicate an ancient origin of Austroasiatic groups\" -sort \"Date Released\" |  ${HOME}/edirect/efetch -format fasta >> /content/drive/MyDrive/RetrieveData/OldCountryFasta/Laos_Thai.fasta"]},{"cell_type":"markdown","metadata":{"id":"5ZhSEbQjuQmp"},"source":["A problem of downloading the data from a paper \"Complete mitochondrial genomes of Thai and Lao populations indicate an ancient origin of Austroasiatic groups\" is that all mtDNA sequences when downloading Ire mixed betIen Thailand and Lao countries. To separate the Thailand sequences from Lao sequences so that I can save them into the correct country file, I ran a function `splitLaosThai` below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfA39IdDatd3"},"outputs":[],"source":["# Splitting up Lao and Thai from the paper and save them. Add only non-existed sequences\n","from Haplogroup.finalCodes.DataWrangling import splitLaosThai\n","splitLaosThai.splitLaosThai(\"/content/drive/MyDrive/RetrieveData/OldCountryFasta/Laos_Thai.fasta\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNX4t1GXQ0q2"},"outputs":[],"source":["%%bash\n","# downloading missing data of Thailand\n","for str in \"Mitochondrial DNA polymorphisms in Thailand\" \"The Peopling of Korea Revealed by Analyses of Mitochondrial DNA and Y-Chromosomal Markers AND Thailand\" \"The Population History and Cultural Dispersal Pattern of Hanging\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Thailand\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"ogigvxd1_gad"},"source":["**Laos** </br>\n","New Data:\n","- 21333001.Southeast Asian diversity: first insights into the complex mtDNA structure of Laos.Bodner et al. (2011): 214"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M_W7Bi3RmK0"},"outputs":[],"source":["# downloading missing data of Laos\n","!source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"Southeast Asian diversity: first insights into the complex mtDNA structure of Laos\" \"Laos\""]},{"cell_type":"markdown","metadata":{"id":"EaQvWDtD52JL"},"source":["**Timor-Leste** </br>\n","New data:\n","+ Genetic admixture history of Eastern Indonesia as revealed by Y-chromosome and mitochondrial DNA analysis.Mona et al. (2009): Despite no clear location/country on the sequences' names, this article mentions country Timor (330 files)\n","+ 25757516.Gomes et al. (2015).txt: this study has 324 files which are all from East Timor (Timor-Leste) (KJ655583-KJ655889: D-loop, KJ676774-KJ676790: complete genome) (keywords: Human settlement history betIen Sunda and Sahul: a focus on East Timor (Timor-Leste) and the Pleistocenic mtDNA diversity)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgOgk5mLSDXd"},"outputs":[],"source":["%%bash\n","# downloading missing data of Timor-Leste\n","for str in \"Genetic admixture history of Eastern Indonesia as revealed by Y-chromosome and mitochondrial DNA analysis\" \"Human settlement history betIen Sunda and Sahul: a focus on East Timor (Timor-Leste) and the Pleistocenic mtDNA diversity\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Timor-Leste\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"kAVgVjMh5_Qm"},"source":["**Viet Nam** </br>\n","New data:\n","+ 21219640.Inland post-glacial dispersal in East Asia revealed by mitochondrial haplogroup M9a'b: Myanmar, Vietnam: keywords for VN: HM346881, HM346883, HM346885, HM346886, HM346889\n","+ .Direct Submission.VN.Phan et al. (2016): Vietnam (DQ834255, DQ834258)\n","+ 19148289.The Peopling of Korea Revealed by Analyses of Mitochondrial DNA and Y-Chromosomal Markers.Jin et al. (2009): Vietnam (84, keyword: The Peopling of Korea Revealed by Analyses of Mitochondrial DNA and Y-Chromosomal Markers and Viet Nam)\n","+ 20513740.Tracing the Austronesian footprint in Mainland Southeast Asia: a perspective from mitochondrial DNA.Peng et al. (2010): 335 (Cham+Kinh)\n","+ '.Direct Submission.Phan et al. (2016)': there are 10 files for VN when searching for keyword “Phan (2016) AND Homo sapiens AND mitochondrion”. Among them already existed 2 files. They dont have a title for this so I still cannot find the article and there is no explanation for the isolate; the isolate only has “VN”"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRyYfOJhSrp1"},"outputs":[],"source":["%%bash\n","# downloading missing data of Vietnam\n","for str in \"HM346881\" \"HM346883\" \"HM346885\" \"HM346886\" \"HM346889\" \"DQ834255\" \"DQ834258\" \"The Peopling of Korea Revealed by Analyses of Mitochondrial DNA and Y-Chromosomal Markers AND Viet Nam\" \"Tracing the Austronesian footprint in Mainland Southeast Asia: a perspective from mitochondrial DNA\" \"Phan (2016) AND Homo sapiens AND mitochondrion\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Viet Nam\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"0mvGugR_6geN"},"source":["**Malaysia** </br>\n","1. Search term \"Single, rapid coastal settlement of Asia revealed by analysis of complete mitochondrial genomes AND Malaysia \":\n","- Missing data if Malaysia is 267 files but all are control region\n","2. Search term \"Single, rapid coastal settlement of Asia revealed by analysis of complete mitochondrial genomes AND Malay\":\n","- Miss 4 complete genome of Malaysia: </br>\n","2: 9_N21(Tor57), 10_M21c(Tor61): Aboriginal Malay (Semelai) (using key word Malay) </br>\n","2: 7_N22(Tor55), 12_M22(Tor63): Aboriginal Malay (Temuan) (using key word Malayu)\n","3. 16982817.Hill et al. (2006): 6 files and one of them which is ORA131B already existed but the others did not (keywords: Phylogeography and ethnogenesis of aboriginal Southeast Asians AND Malaysia)\n","4. 22729749.Evolutionary history of continental southeast asians: 'early train' hypothesis based on genetic analysis of mitochondrial and autosomal\n","DNA data.Jinam et al. (2012): Malay: 86 genome: 23 Bidayuh (BD); 24 Jehai (JH); 21 Seletar (SL); 18 Temuan (TM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh_4AX1YTYBX"},"outputs":[],"source":["%%bash\n","# downloading missing data of Malaysia\n","for str in \"Single, rapid coastal settlement of Asia revealed by analysis of complete mitochondrial genomes AND Malaysia\" \"Single, rapid coastal settlement of Asia revealed by analysis of complete mitochondrial genomes AND Malay\" \"Single, rapid coastal settlement of Asia revealed by analysis of complete mitochondrial genomes AND Malayu\" \"Phylogeography and ethnogenesis of aboriginal Southeast Asians AND Malaysia\" \"Evolutionary history of continental southeast asians: 'early train' hypothesis based on genetic analysis of mitochondrial and autosomal DNA data\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Malaysia\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"Wmbl5nhf7F_z"},"source":["**Indonesia** </br>\n","New Data:\n","- '21407194.Larger mitochondrial DNA than\n","Y-chromosome differences betIen.Gunnarsdottir et al. (2011)'. Key word: Larger mitochondrial DNA than Y-chromosome differences betIen matrilocal and patrilocal groups from Sumatra (72 files)\n","- 21407194.Gunnarsdottir et al. (2011): HM596654\n","- 16982817.Hill et al. (2006): 97 files and 4 of them existed (DQ981465-68), but the others did not (keyword: Phylogeography and ethnogenesis of aboriginal Southeast Asians AND Indonesia)\n","- Unpublished.Ngili et al. (2009).txt: all 206 files from Indonesia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKhDhEkPT-2S"},"outputs":[],"source":["%%bash\n","# downloading missing data of Indonesia\n","for str in \"Larger mitochondrial DNA than Y-chromosome differences betIen matrilocal and patrilocal groups from Sumatra\" \"HM596654\" \"Phylogeography and ethnogenesis of aboriginal Southeast Asians AND Indonesia\" \"Ngili (2009)\"; do\n","source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"$str\" \"Indonesia\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"z5Q4PumwBPsz"},"source":["**Cambodia** </br>\n","New Data:\n","- Keywords: \"Analysis of mitochondrial genome diversity identifies new\" (1248 seqs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAJla3aLCnFJ"},"outputs":[],"source":["# downloading missing data of Cambodia\n","! source /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getMissingData.sh; getMissingData \"Analysis of mitochondrial genome diversity identifies new\" \"Cambodia\""]},{"cell_type":"markdown","metadata":{"id":"a_OcRwYx8Vl7"},"source":["After collecting the missing data, check again the number of sequences in each country fasta file in the oldCountryFasta folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUvN7kjbfnwt"},"outputs":[],"source":["# check the number of sequences in each country fasta file\n","%%bash\n","DataList=/content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt\n","Field_Separator=$IFS\n","IFS=,\n","for val in `cat $DataList`\n","do echo $val; cat /content/drive/MyDrive/RetrieveData/OldCountryFasta/$val.fasta | grep \">\" | wc -l; done"]},{"cell_type":"markdown","metadata":{"id":"plQdwLQAHTW0"},"source":["### **2. Data wrangling**\n","\n","Caveat:\n","- As I mentioned at the caveats above, a number of sequences of Dataset 1 and Dataset 2 below are the new numbers when I re-ran the codes. At the Dataset 3 section, I ran a function to filter only 4932 original sequences that I used in our paper."]},{"cell_type":"markdown","metadata":{"id":"JexAHlZJIoRF"},"source":["#### **Dataset 1**\n","- After getting all the raw data of 11 countries above, I checked if there Ire any duplicated sequences in each country fasta file and also betIen the 11 countries, and made sure I only took the unique sequences. I removed the sequences have the accession numbers on their name that appeared more than once and then saved the unique accession number sequences in a file called \"UniqueAccNumForDataset1.txt\".\n","- Datatset 1 still contains the reference, D-loop, non-homosapien, etc. sequences."]},{"cell_type":"markdown","metadata":{"id":"0Lz4ky5XQB_K"},"source":["  First of all, checking how many raw sequences before removing the duplicated ones by running the bash script below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAK6vk0kX_Go","outputId":"c899c8bb-e835-48e3-9024-1bdb8ff76950"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of raw seqs of 11 countries:  9660\n"]}],"source":["%%bash\n","# create a list of all sequences in original data of 11 countries\n","DataList=/content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt\n","Field_Separator=$IFS\n","IFS=,\n","for country in `cat $DataList`\n","do cat /content/drive/MyDrive/RetrieveData/OldCountryFasta/\"$country\".fasta | grep \">\" ; done > /content/drive/MyDrive/Haplogroup/finalCodes/others/allSeq.txt\n","# check number of all raw seqs\n","echo \"Number of raw seqs of 11 countries: \" `cat /content/drive/MyDrive/Haplogroup/finalCodes/others/allSeq.txt | wc -l`"]},{"cell_type":"markdown","metadata":{"id":"TDKBqtoTQZo_"},"source":["After knowing that there are 9660 raw sequences, I started to run the `checkDuplicated` function to count how many unique accession number sequences and how many duplicated ones."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJwoT_maN5D6","outputId":"2f3abf67-659b-4615-ed3e-546493efc851"},"outputs":[{"data":{"text/plain":["(9605, 55)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# check no duplicated files in a allseqsFile\n","from Haplogroup.finalCodes.DataWrangling.Dataset1 import checkDuplicated\n","# run function\n","uniq, dupl = checkDuplicated.checkDuplicated()\n","len(uniq), len(dupl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffwNFKgrbuO8"},"outputs":[],"source":["# save the unique list of accnum for dataset 1\n","saveFile.saveFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset1.txt\", \",\".join(uniq))"]},{"cell_type":"markdown","metadata":{"id":"e1Za_tvTQoY9"},"source":["There Ire 9605 unique sequences after removing the duplicated ones. Then I saved the unique accession numbers of these sequences at /content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset1.txt which was used for the next step of filtering to get Dataset 2."]},{"cell_type":"markdown","metadata":{"id":"Y3_GTEYtRZoK"},"source":["The bash script below used the input which is the file \"UniqueAccNumForDataset1.txt\" to gain the brief information of each sequence (the authors, title, pubmed, isolate name, organism). This information was used later when doing the Dataset 3 filtering, and making the tables.\n","\n","After running the bash script, the information of sequences of Dataset 1 was saved at /content/drive/MyDrive/Haplogroup/finalCodes/others/Dataset1_accnumInfo.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOljb1HTM-Fs"},"outputs":[],"source":["# get information of all the unique sequences of dataset 1\n","! bash /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/Dataset1AccnumInfo.sh"]},{"cell_type":"markdown","metadata":{"id":"lmmbGm1lJrAN"},"source":["#### **Dataset 2**\n","\n","- After having Dataset 1, I ran function `Dataset2` to remove reference, non-homosapiens sequences.\n","\n","- This function used the file \"UniqueAccNumForDataset1.txt\" to read the information of sequences of Dataset 1, remove the reference genome, non-homosapiens, and D-loop sequences, and finally save the others in a new file at /content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset2.txt. </br>\n","Ater running, there Ire only 7082 sequences in the Dataset 2.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mncH2fCYklMI","outputId":"15cd76d1-8567-43f4-a7f4-42555c32ecf0"},"outputs":[{"data":{"text/plain":["7082"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# calling function\n","from Haplogroup.finalCodes.DataWrangling.Dataset2 import Dataset2\n","UniqListDataset2, RemovedData2 = Dataset2.Dataset2()\n","len(UniqListDataset2)"]},{"cell_type":"markdown","metadata":{"id":"pFga2RRgKTpg"},"source":["#### **Datatset 3**\n","\n","**Setting up before getting into the main below purposes of Dataset 3 section**\n","- After getting Dataset 2, I ran function `Dataset3` to remove control region sequences from Dataset 2, and remain only complete genomes which are saved in a new file /content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset3.txt.\n","\n","- Sequences of Dataset 3 are the final sequences used in this study. The new 5409 sequences Ire the updated sequences after running the codes again, but for our study, I just only took the original Dataset 3 sequences (4932). Therefore, I ran another function to get only 4932 sequences as same as original ones from this updated 5409 sequences."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0t4a0l3mO-M","outputId":"74aec605-e3a5-43b5-9282-54a3198b3d35"},"outputs":[{"data":{"text/plain":["5409"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# calling function\n","from Haplogroup.finalCodes.DataWrangling.Dataset3 import Dataset3\n","UniqListDataset3, RemovedData3 = Dataset3.Dataset3()\n","len(UniqListDataset3)"]},{"cell_type":"markdown","metadata":{"id":"AFkWQgimiVJa"},"source":["Below is the code reading the file \"allSeqsDataset3_Ori.txt\" which contains the names of original 4932 sequences. It checked the existence of them in the updated 5409 sequences, and saved the new filtered Dataset 3 sequences at /content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset3_4932.txt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvVJB1xEN4S0"},"outputs":[],"source":["dataset3 = []\n","for string in openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/allSeqsDataset3_Ori.txt\").split(\"\\n\"):\n","  if len(string)>0:\n","    accnum = string.split(\".\")[2]\n","    if accnum in openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset3.txt\"):\n","      dataset3.append(accnum)\n","saveFile.saveFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/UniqueAccNumForDataset3_4932.txt\", ','.join(dataset3))"]},{"cell_type":"markdown","metadata":{"id":"npVh26IEi55S"},"source":["- The final 4932 sequences at this moment Ire a total number of unique sequences of all 11 countries and Ire named after their unique accession numbers. When I looked at the duplicated sequences, the reason they Ire duplicated is because although I typed the different name of the country when I downloaded the data, there Ire still 2 different countries having the same accession number sequences. For example, in Indonesia, there exists a unique \"GQ119010\" which also exists in Philippines. Moreover, for the next following steps, I wanted to know the country information of the sequence, so I had to decide which country that kind of accession number sequence should belong.\n","- To make this kind of accession number sequences became clearer, I checked if there Ire any overlapping accession numbers betIen 11 countries and saved at file /content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932.txt. After knowing the duplicated ones, I used E-Summary of EntrezDirect to get more specific information from the sequences. I ran the bash script below to download more specific information of these duplicated sequences and saved at file /content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932_moreInfo.txt."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wshlnX1U6JaH","outputId":"9140b8ca-0771-4bb0-b0ec-1878d6b3ab60"},"outputs":[{"data":{"text/plain":["(4932, 37)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Check if there Ire nay duplicated sequences and what Ire they (the output here shows that there Ire 37 duplicated sequences of all 4932 sequences)\n","from Haplogroup.finalCodes.DataWrangling.Miscellaneous import assignCountry\n","list_acc_country, dup_acc_country = assignCountry.assignCountry()\n","len(list_acc_country), len(dup_acc_country)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5wimb_ZFOFk"},"outputs":[],"source":["# save the duplicated seqs to get more info about them\n","saveFile.saveFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932.txt\",','.join(dup_acc_country))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nk5tH8kIDu23"},"outputs":[],"source":["%%bash\n","# a bash script to get more info of duplicates to assign which country should they belong\n","AccList=/content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932.txt\n","Field_Separator=$IFS\n","IFS=,\n","\n","for val in `cat $AccList`\n","do echo $val >> /content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932_moreInfo.txt\n","${HOME}/edirect/esummary -db nuccore -id $val -format medline | egrep \"country\" >> /content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932_moreInfo.txt\n","done"]},{"cell_type":"markdown","metadata":{"id":"tw6lt0xusi_e"},"source":["After knowing more the specific countries that the sequences should be in, I ran the function `uniqAccCountry` to assign the specific country the sequence belongs to. This process is necessary because I need the country information of the sequences to create the following tables containing the detail information of them, and also change their accession number names to the new names."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2oqdFD72c7x"},"outputs":[],"source":["from Haplogroup.finalCodes.DataWrangling.Miscellaneous import uniqAccCountry\n","uniq_acc_country = uniqAccCountry.uniqAccCountry(list_acc_country,\"/content/drive/MyDrive/Haplogroup/finalCodes/others/duplicatedSeqOf4932_moreInfo.txt\",dup_acc_country)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxfn9X8D7pk6"},"outputs":[],"source":["from Haplogroup.finalCodes.DataWrangling import saveFile\n","saveFile.saveFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/uniq_acc_country.txt\",uniq_acc_country)"]},{"cell_type":"markdown","metadata":{"id":"qPXgOQ0-vDl1"},"source":["**In this Dataset 3 section, the main purposes are:**\n","\n","***1. SplitUp:*** Split up all sequences in a country's big fasta file downloaded from Entrez Direct to the small individual files which each file contains each separate sequence. The I saved those small files at the subset folder named after the country of a big fasta file inside the folder Dataset 3. (Format of folder: Dataset3/<\"CountryName\">/)\n","\n","***2. CreateNewName:*** After having the new file of each sequence in the folder, I renamed these sequences based on the format \"Country.Isolate.AccessionNumber.Haplogroup\", and used this new name to change the old name inside the sequence fasta file, and aslo the name of the file.\n","\n","***3. Merging:*** Merge each new labeled sequence of each country into a big file of that country and saved at \"Dataset3/<\"CountryName\">/<\"CountryName\">_NewBigFile.fasta."]},{"cell_type":"markdown","metadata":{"id":"0URxE1hq3S5F"},"source":["Before doing the above purposes, I had to create Dataset3 foler and also subset folders inside which their name based on the names of 11 countries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gswj51hDDOLC"},"outputs":[],"source":["%%bash\n","# create Dataset3 folder and its subset folders\n","mkdir /content/drive/MyDrive/RetrieveData/Dataset3/\n","\n","DataList=/content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt\n","Field_Separator=$IFS\n","IFS=,\n","for val in `cat $DataList`\n","do mkdir /content/drive/MyDrive/RetrieveData/Dataset3/\"$val\"/\n","done"]},{"cell_type":"markdown","metadata":{"id":"hPXeEw577xoe"},"source":["1. SplitUp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zl766g17wcC"},"outputs":[],"source":["from Haplogroup.finalCodes.DataWrangling.Dataset3 import splitSeq\n","s = splitSeq.splitSeq()\n","s.splitFastaSeq(uniq_acc_country)\n","! ls /content/drive/MyDrive/RetrieveData/Dataset3/*/* > /content/drive/MyDrive/Haplogroup/finalCodes/others/nameAccSeqsDataset3.txt\n","saveFile.saveFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/nameAccSeqsDataset3.txt\", ','.join(openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/nameAccSeqsDataset3.txt\").split(\"\\n\")[:-1]))"]},{"cell_type":"markdown","metadata":{"id":"DdbgmXgh7zJp"},"source":["2. CreateNewName"]},{"cell_type":"markdown","metadata":{"id":"KIIHi3cw5R96"},"source":["Because I wanted to create a new name having the haplogroup, firstly I had to run Haplogrep and create a file saving the information of these haplogroups."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZxeN9jfo3Ry"},"outputs":[],"source":["# Download haplogrep\n","!curl -sL haplogrep.now.sh | bash"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeeKblVv4aqy"},"outputs":[],"source":["!bash /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/haplogroup.sh"]},{"cell_type":"markdown","metadata":{"id":"xz5kagPk749x"},"source":["Create a new name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VDRMQTcZNUp"},"outputs":[],"source":["from Haplogroup.finalCodes.DataWrangling.Dataset3 import splitSeq\n","s = splitSeq.splitSeq()\n","listOfNewName = []\n","! ls /content/drive/MyDrive/RetrieveData/Dataset3/*/* > /content/drive/MyDrive/Haplogroup/finalCodes/others/nameAccSeqsDataset3.txt\n","for name in openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/nameAccSeqsDataset3.txt\").split(\"\\n\")[:-1]:\n","  newName = s.createNewName(name)\n","  listOfNewName.append(newName)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ipVEw3AW4_G"},"outputs":[],"source":["# after create new file with new name, remove the accnumNameFile\n","%%bash\n","DataList=/content/drive/MyDrive/Haplogroup/finalCodes/others/nameAccSeqsDataset3.txt\n","Field_Separator=$IFS\n","IFS=,\n","for val in `cat $DataList`\n","do rm \"$val\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"JTf0AWwV43ds"},"source":["3. Merging"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blx2zEjTgLI4","outputId":"d7baa769-b655-4525-db00-c625909dc5ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Brunei done\n","Cambodia done\n","Indonesia done\n","Laos done\n","Malaysia done\n","Myanmar done\n","Philippines done\n","Singapore done\n","Thailand done\n","Timor-Leste done\n","Viet Nam done\n"]}],"source":["! ls /content/drive/MyDrive/RetrieveData/Dataset3/*/* > /content/drive/MyDrive/Haplogroup/finalCodes/others/newNameSeqsDataset3_4932.txt\n","s = splitSeq()\n","AllNewSeqList = openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/newNameSeqsDataset3_4932.txt\").split(\"\\n\")[:-1]\n","for country in openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt\").split(\",\"):\n","  s.mergeSeqsBasedOnCountry(AllNewSeqList,country)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4Ugt7UsI5tn","outputId":"e036ab07-d771-4907-ab01-2f23b4286b15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Brunei\n","9\n","Cambodia\n","399\n","Indonesia\n","657\n","Laos\n","60\n","Malaysia\n","152\n","Myanmar\n","152\n","Philippines\n","447\n","Singapore\n","2\n","Thailand\n","2340\n","Timor-Leste\n","18\n","Viet Nam\n","707\n"]}],"source":["# check the number of sequences in each country fasta file\n","%%bash\n","DataList=/content/drive/MyDrive/Haplogroup/finalCodes/others/countries.txt\n","Field_Separator=$IFS\n","IFS=,\n","for val in `cat $DataList`\n","do echo $val; ls /content/drive/MyDrive/RetrieveData/Dataset3/$val/ | wc -l; done"]},{"cell_type":"markdown","metadata":{"id":"cUW9g13aRZh-"},"source":["### **3. Tables**\n","\n","In this section, I created tables particularly a table Isolate Explantion which contains the main information of 4932 sequences in this study. Before doing that, I had to do the below set up to get the brief information of the sequences."]},{"cell_type":"markdown","metadata":{"id":"2lkduxuVBt3A"},"source":["**Setting up before creating table**\n","-  ***Getting year:*** From the original file \"Dataset1_accnumInfo.txt\", I filtered out the accession numbers of Dataset3 and put their information into \"Dataset3_accnumInfo.txt\". Besides this file, I realized that for the Isolate Explanation table, I also wanted the published year of the reference papers, so I ran again esummary from Entrez Direct and saved the year information to a new file /content/drive/MyDrive/Haplogroup/finalCodes/others/YEAR_Data3.txt.\n","- ***Getting polymorphism:*** I also wanted to take polymorphism of each sequence but I just ran Haplogrep above and took the Haplogroup name without polymorphism, so I ran again Haplogrep but with the extend parameter and save these polymorphisms into new file."]},{"cell_type":"markdown","metadata":{"id":"Bms9AjFOkObv"},"source":["***Getting year***\n","\n","Two kinds of year:\n","1. Year from published papers (PubDate)\n","2. Year at LOCUS (if I cannot find the year of published papers):\n","- For Direct Submission sequences, there might be some submissions still having the title of the paper, and I called this case \"Unpublished\".\n","- If the sequences are Direct Submitted but no paper's title then I called this case \"Direct Submission\"."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbaknZsfzY0h","outputId":"99257b80-0249-420b-9b79-b2f6ea71fee9"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["from Haplogroup.finalCodes.DataWrangling.Dataset3 import yearFor4932\n","y = yearFor4932.YearFor4932()\n","pubID, noID = y.createYearInfo4932()\n","# run bash script to get year from LOCUS and also PUBDATE\n","! bash /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/getYearInfo.sh\n","# create a year file after having year from LOCUS and PUBDATE\n","y.createYearFile(pubID)"]},{"cell_type":"markdown","metadata":{"id":"86yrz_bakJCl"},"source":["***Getting polymorphism***\n","\n","Caveat: The folder \"Viet Nam\" had the new names of the sequences containing the words \"Viet Nam\" which has the space betIen Viet and Nam. When Haplogrep reads the sequences, it splits up the space and only keeps the first word before the space (\"Viet\"). To fix this problem, I created a new folder Vietnam and saved again all the files from Viet Nam folder to this folder with the new names of files and sequences inside the files from \"Viet Nam\" to \"Vietnam\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lURe20A0cMe"},"outputs":[],"source":["! mkdir /content/drive/MyDrive/RetrieveData/Dataset3/Vietnam\n","! ls /content/drive/MyDrive/RetrieveData/Dataset3/Viet*/* >> /content/drive/MyDrive/Haplogroup/finalCodes/others/FixVietnam.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Fs0BIU60QD1","outputId":"de6765ba-b8dc-4486-d5a5-702bf3a80ca2"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["from Haplogroup.finalCodes.DataWrangling.Miscellaneous import fixSpaceBetIenVietnam\n","fixSpaceBetIenVietnam.fixSpaceBetIenVietnam()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLQ2mArxzmvX"},"outputs":[],"source":["# remove the old file newNameSeqsDataset3_4932.txt having the sequences including the words \"Viet Nam\"\n","! rm /content/drive/MyDrive/Haplogroup/finalCodes/others/newNameSeqsDataset3_4932.txt\n","\n","# remove the old folder \"Viet Nam\" after having the new one \"Vietnam\"\n","! rm -r \"/content/drive/MyDrive/RetrieveData/Dataset3/Viet Nam\"\n","\n","# get the name of all 4932 sequences for the polymorphism running below\n","! ls  /content/drive/MyDrive/RetrieveData/Dataset3/*/* >> /content/drive/MyDrive/Haplogroup/finalCodes/others/newNameSeqsDataset3_4932.txt\n","\n","# run bash script to get polymorphism\n","! bash /content/drive/MyDrive/Haplogroup/finalCodes/bashScriptCodes/Get4932polymorphism.sh"]},{"cell_type":"markdown","metadata":{"id":"AbJmgAMTeNH2"},"source":["#### **Isolate Explanation Table**"]},{"cell_type":"markdown","metadata":{"id":"KE-p5vJAvFqe"},"source":["After getting all 4932 sequences of 11 countries in South East Asia, I created a final table \"CompleteFullIsoTab\" which contains the information of the sequences.\n","1. Firstly, I made a draft table or I called IsolateExplanation which includes the columns:\n","- ID: The ascending order starting from 0 to 4931\n","- Reference: The combination of information of 4 columns: pubmedID, title, Author(s), and year.\n","- pubmedID: The pubmedID of the paper. HoIver, for the special situations:\n","the author(s) directly submitted the sequences without any paper's title, then pubmedID is a blank space; there was a title of a paper but I could not find it, then I assumed that the paper was unpublished, so I wrote \"Unpublished\".\n","- title: The title of the paper. For the sequences that I could not find the paper and at its summary, and the only title was\"Direct Submission\", so I just kept \"Direct Submission\".\n","- year: The year that the paper published. For the case of Direct Submission or Unpublished papers, I used the year at LOCUS site of summary on nuccore NCBI.\n","- Author(s): If there is more than one author, I used et al behind the first author's name. If there is only one, then I just wrote that author's name\n","- AccessionNumber: Accession number of the sequence.\n","- name: the name of the sequence which was labelled in a format \"Country.Isolate.AccesssionNumber.Haplogroup\".\n","- Country: The country where the sequences' samples Ire obtained.\n","- Isolate: The isolate name that is written at the NCBI summary of the sequence.\n","- Explanation: To explain the meaning of the isolate name (I will explain detailedly this column later below).\n","- Location: The living/born location of the sequence's sample.\n","- Language: The language that sequence's sample might speak.\n","- Population: The population that sequence's sample might belong to.\n","- haplo: The specific haplogroup name getting from both the paper of the sequences or the Haplogrep tool.\n","- haplogroup1: The first capital letter of haplogroup in haplo column.\n","- haplogroup2: The capital letter(s) and next positive number (if the number is not next to the capital letter(s), I took the next single character such as \"+\").\n","- haplogroup3: The uppercase letter(s), next positive number or next single character, and next loIrcase letter (if exist) or next number(1-100) if before it is not number but a single character such as \"+\".\n","- Polymorphism: Polymorphism of the sequence getting from Haplogrep tool."]},{"cell_type":"markdown","metadata":{"id":"St6xEInbU2Ug"},"source":["I also created a file Haplogroup/finalCodes/Tables/translation.txt which includes specific information of samples of published sequences such as their locations, or ethnicities, or population, etc.. By reading briefly the paper of those sequences, I assumed that the isolate names of the sequences from the papers might be the codes or labels that tells us the name of ethnicity or location of the samples of sequences. Therefore I created column \"Explanation\" to write down the meaning of these isolate names. For sequences that I could not find the ehtnicity or location information from the isolate names or even papers, I kept the isolate name at the Explanation column but transform them to a more literate name, for example \"Thai755\" was changed to \"Thai\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAG_AhtohYuG"},"outputs":[],"source":["# create list of names of 4932 sequences to make the Original IsoTab\n","from Haplogroup.finalCodes.DataWrangling import openFile, saveFile\n","rawList = openFile.openFile(\"/content/drive/MyDrive/Haplogroup/finalCodes/others/newNameSeqsDataset3_4932.txt\").split(\"\\n\")[:-1]\n","listOfNames = list(map(lambda x: x.split(\"/\")[-1].split(\".fasta\")[0], rawList))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQvOuw4xDwtx","outputId":"ecdbb2d5-45d8-4f07-98c3-eaa995b7337d"},"outputs":[{"data":{"text/plain":["4932"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["len(listOfNames) # I can see the length of the list is 4932 which is also a number of sequences I used to create the below Isolate Explanation table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QXchEXZD95-"},"outputs":[],"source":["# create table IsolateExplantion.xlsx\n","from Haplogroup.finalCodes.Tables.IsoTab import explainIso\n","output = explainIso.explainIso(listOfNames)\n","output.to_excel('/content/drive/MyDrive/RetrieveData/tables/IsolateExplanation.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"KS2HEa0CLvy3"},"source":["After that, I created CompleteFullIsoTab table from IsolateExplanation table by making some changes:\n","- Present or ancient: I added this column betIen columns \"year\" and \"Author(s)\". This column classified if the papers of the sequences researched about the ancient sequences (such as getting the samples from archeology sites) or the modern sequences (getting samples from modern human: buccal or blood samples). For the direct submitted sequences and unpublished paper of sequences, I wrote unknown, but on the NCBI nuccore summary of sequences, if they mentioned about the samples such as from human tissue, then I assumed there might be modern samples.\n","- Ethnicity: Column \"Population\" was replaced by \"Ethnicity\" column and I placed this new column betIen \"Explanation\" and \"Location\" columns. This column shows the ethnicity that I assumed the people of the sequences belong to.\n","- Language family: I added this new column behind \"Language\" column. After knowing the language people might talk based on their ethnicity, I used extra sources (links of extra sources are in a file \"/content/drive/MyDrive/RetrieveData/tables/sources.txt\") to search for the language family of the language.\n","\n","*Notice: Besides that, I still manually kept looking for the more speicifc and right information particuclarly for the columns Ethnicity, Location, Language and Language family. This is because after reading careful again the papers and also searching for more sources outside the papers, I got more information to replaced the not specific ones.*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hCxlsV2PxyS2"},"source":["After having more specific and updated information in the table CompleteFullIsoTab, I used this table to create sub tables (more details at section **Table1**, **Table2**, **Table3** below)."]},{"cell_type":"markdown","metadata":{"id":"5RQ7xMEqfVH2"},"source":["#### **Table1**\n","\n","- In this section Table1, our sub tables Ire created for the purpose of summarizing only the unique information of \"Explanation\", \"Ethnicity\", \"Location\", \"Language\", and \"Language family\" from the big table CompleteFullIsoTab. The columns of table1 are: \"ID\", \"Country\", \"References\", \"Explanation\", \"Ethnicity\", \"Location\", \"Language\", \"Language family\", \"Sample size\", and other halplogroup names.\n","- Briefly, for each country, if 2 different sequences had the different explanations, or had the same explanations but the differences betIen one of 3 columns: \"Ethnicity\", \"Location\", and \"Language family\", then I still considered as a new unique value and added the new row for this. After that, for each new unique rows, I counted how many sequences that also had the same country, explanation, ethnicity, location, and language family, and put these counted numbers at \"Sample size\" column. Behind \"Sample size\" column are the columns which Ire named after the name of each different haplogroup. The numbers in those columns shows a number of sequences having the same Country, Explanation, Ethnicity, Location, Language family and the same name of Haplogroup. Finally for References columns, because there can be more than 1 paper having the same information of the samples, I added a comma betIen those references.\n","\n","Using this kind of aforementioned format for the all the tables in section Table1, I created:\n","- 11 tables of 11 countries in SEA.\n","- A big file putting all the small 11 countries' tables together and saved in 2 files: \"SEA_haplgroups.csv\", and \"Changed_SEA_haplogroups.xlsx\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_OXJS7tzImu"},"outputs":[],"source":["from Haplogroup.finalCodes.Tables.tables import table1\n","import pandas as pd\n","IsoTab = pd.read_excel(\"/content/drive/MyDrive/RetrieveData/tables/CompleteFullIsoTab.xlsx\")\n","! mkdir /content/drive/MyDrive/RetrieveData/tables/table1\n","countries = \"Brunei Cambodia Indonesia Laos Malaysia Myanmar Philippines Singapore Thailand Timor-Leste Vietnam\"\n","for country in countries.split():\n","  data = table1.createTable1(country, IsoTab)\n","  data.to_csv('/content/drive/MyDrive/RetrieveData/tables/table1/'+country+'1.csv')\n","  print(country,'finish')\n","# merge all 11 countries\n","countries = 'Cambodia Indonesia Laos Malaysia Myanmar Philippines Singapore Thailand Timor-Leste Vietnam'\n","df = pd.read_csv('/content/drive/MyDrive/RetrieveData/tables/table1/Brunei1.csv', index_col=\"ID\")\n","for country in countries.split():\n","  df1 = pd.read_csv('/content/drive/MyDrive/RetrieveData/tables/table1/'+country+'1.csv', index_col=\"ID\")\n","  df = pd.concat([df, df1], ignore_index=True, sort=False)\n","# save the total 11 countries files\n","df = df.fillna(0)\n","df = df.replace(0,'-')\n","df.to_csv('/content/drive/MyDrive/RetrieveData/tables/table1/SEA_haplogroups.csv')\n","df.to_excel('/content/drive/MyDrive/RetrieveData/tables/table1/Changed_SEA_haplogroups.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"xZ2Jpe21ftga"},"source":["#### **Table2**"]},{"cell_type":"markdown","metadata":{"id":"mz4ubQaLLvy5"},"source":["There are 2 main outputs of section Table2 after running the cell box below:\n","1. Eleven tables showing the frequency of haplogroups of eleven SEA countries\n","2. A big Haplofrequency table includes the total of the haplogroups of all 11 countries and their frequency of a total of 4932 sequences.\n","\n","All the tables above have the same columns:\n","- Haplogroup (name of haplogroup)\n","- Number of Individuals (count the appearance of that haplogroup)\n","- Frequency (the percentage of that haplogroup in the total of the number of all haplogroups in that country for first output and in 11 countries for second output)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_7O2BfFYvgj","outputId":"9cfa5a56-671f-4737-ed7f-2f5e9afc173b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Brunei finish\n","Cambodia finish\n","Indonesia finish\n","Laos finish\n","Malaysia finish\n","Myanmar finish\n","Philippines finish\n","Singapore finish\n","Thailand finish\n","Timor-Leste finish\n","Vietnam finish\n"]}],"source":["from Haplogroup.finalCodes.Tables.tables import table_2\n","! mkdir /content/drive/MyDrive/RetrieveData/tables/table2\n","! mkdir /content/drive/MyDrive/RetrieveData/tables/table2/countries\n","countries = \"Brunei Cambodia Indonesia Laos Malaysia Myanmar Philippines Singapore Thailand Timor-Leste Vietnam\"\n","# table 2 for each country\n","for country in countries.split():\n","  nameFile = \"/content/drive/MyDrive/RetrieveData/tables/table1/\"+country + \"1.csv\"\n","  data = table_2.Table2(nameFile)\n","  data.to_csv('/content/drive/MyDrive/RetrieveData/tables/table2/countries/'+country+'2.csv')\n","  print(country,'finish')\n","# table 2 for all SEA countries\n","df = table_2.Table2(\"/content/drive/MyDrive/RetrieveData/tables/SEA_haplogroups.csv\")\n","df.to_csv('/content/drive/MyDrive/RetrieveData/tables/table2/Haplofrequency.csv')"]},{"cell_type":"markdown","metadata":{"id":"GIqywxTSgCvV"},"source":["#### **Table3**\n","\n","In this Table3 section I created 4 main outputs:\n","1. table3a_CountryAndEthnicity:\n","> - Haplogroup column (names of Haplogroup)\n","> - Ethnicities of 11 countries which each country was highlighted in different colors. </br>\n","> - In each country, there Ire : </br>\n",">> - A Total column (the total numbers of that specific halogroup / the total number of all types of haplogroups in that country). </br>\n",">> - Next to Total column is the name of each different ethnicity appearing in that country (the total number of the specific haplogroup in the specific ethnicity/the total number of all types of haplogroups in that specific ethnicity).\n","2. table3b_Ethnicity:\n","> - Haplogroup column (names of Haplogroup)\n","> - A Total column (the total numbers of that specific haplogroup / the total number of all types of haplogroups). </br>\n","> - Next to Total column is the name of each different ethnicity appearing in all 11 countries (the total number of the specific haplogroup in the specific ethnicity/the total number of all types of haplogroups in that specific ethnicity).\n","3. table3c_LanguageFamily: </br>\n","The same format of columns as that of table3b_Ethnicity, but changed the independent variable from Ethnicity to Language Family.\n","4. table3d_CountryAndLanguageFamily: </br>\n","The same format of columns as that of table3a_CountryAndEthnicity, but changed the independent variable from Ethnicity to Language Family."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9u9oKscaOi9K"},"outputs":[],"source":["! mkdir /content/drive/MyDrive/RetrieveData/tables/table3\n","! mkdir /content/drive/MyDrive/RetrieveData/tables/table3/countries"]},{"cell_type":"markdown","metadata":{"id":"QTE5EhOwgiUo"},"source":["table 3a_CountryAndEthnicity"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLdoFGULLvy6","outputId":"d534c56b-1161-48fa-e436-4a7fb337e972"},"outputs":[{"name":"stdout","output_type":"stream","text":["Brunei finish\n","Cambodia finish\n","Indonesia finish\n","Laos finish\n","Malaysia finish\n","Myanmar finish\n","Philippines finish\n","Singapore finish\n","Thailand finish\n","Timor-Leste finish\n","Vietnam finish\n"]}],"source":["# call function\n","from Haplogroup.finalCodes.Tables.tables.table3 import createTable3ad\n","import pandas as pd\n","# run function for Ethnicity\n","countries = 'Brunei Cambodia Indonesia Laos Malaysia Myanmar Philippines Singapore Thailand Timor-Leste Vietnam'\n","data = ''\n","groups = ['Country','Ethnicity']\n","for country in countries.split():\n","  df = createTable3ad.createTable3ad(country,groups,\"/content/drive/MyDrive/RetrieveData/tables/SEA_haplogroups.csv\",groups[-1])\n","  df.to_csv(\"/content/drive/MyDrive/RetrieveData/tables/table3/countries/\"+country+\"3.csv\")\n","  if len(data) < 1:\n","    data = df\n","  else:\n","    add = df.drop(['Haplogroup'],axis=1)\n","    data = pd.concat([data,add],axis=1)\n","  print(country,'finish')\n","data.to_excel(\"/content/drive/MyDrive/RetrieveData/tables/table3/table3a_CountryAndEthnicity.xlsx\")"]},{"cell_type":"markdown","metadata":{"id":"QzBERiG4Lvy6"},"source":["table3d_CountryAndLanguageFamily"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdUZrl2U-Xk7","outputId":"3f85f206-35c6-427e-dba8-db29a03fad1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Brunei finish\n","Cambodia finish\n","Indonesia finish\n","Laos finish\n","Malaysia finish\n","Myanmar finish\n","Philippines finish\n","Singapore finish\n","Thailand finish\n","Timor-Leste finish\n","Vietnam finish\n"]}],"source":["# call function\n","from Haplogroup.finalCodes.Tables.tables.table3 import createTable3ad\n","# run function for Language Family\n","countries = 'Brunei Cambodia Indonesia Laos Malaysia Myanmar Philippines Singapore Thailand Timor-Leste Vietnam'\n","data = ''\n","groups = ['Country','Language family']\n","for country in countries.split():\n","  df = createTable3ad.createTable3ad(country,groups,'/content/drive/MyDrive/RetrieveData/tables/SEA_haplogroups.csv',groups[-1])\n","  if len(data) < 1:\n","    data = df\n","  else:\n","    add = df.drop(['Haplogroup'],axis=1)\n","    data = pd.concat([data,add],axis=1)\n","  print(country,'finish')\n","data.to_excel('/content/drive/MyDrive/RetrieveData/tables/table3/table3d_CountryAndLanguageFamily.xlsx')"]},{"cell_type":"markdown","metadata":{"id":"_0eF3iRB-dbA"},"source":["Table 3b,c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQmi3iOl-ki8"},"outputs":[],"source":["# call function\n","from Haplogroup.finalCodes.Tables.tables.table3 import createTable3bc\n","# run function to get table3b_Ethnicity\n","groups = ['Ethnicity']\n","data = createTable3bc.createTable3bc(groups,'/content/drive/MyDrive/RetrieveData/tables/SEA_haplogroups.csv','Ethnicity')\n","data.to_excel('/content/drive/MyDrive/RetrieveData/tables/table3/table3b_Ethnicity.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmmtQdK6-qlG"},"outputs":[],"source":["# run function to get table3c_LanguageFamily\n","from Haplogroup.finalCodes.Tables.tables.table3 import createTable3bc\n","groups = ['Language family']\n","data = createTable3bc.createTable3bc(groups,'/content/drive/MyDrive/RetrieveData/tables/SEA_haplogroups.csv','Language family')\n","data.to_excel('/content/drive/MyDrive/RetrieveData/tables/table3/table3c_LanguageFamily.xlsx')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
